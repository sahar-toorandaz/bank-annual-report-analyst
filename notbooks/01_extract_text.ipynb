{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b8d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(\"Key loaded:\", OPENAI_API_KEY is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4364aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "pdf_path = Path(\"../data/raw/TD-2024-Annual-Report.pdf\")\n",
    "\n",
    "pagenumber_text = []\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for i , page in enumerate (pdf.pages):\n",
    "        text = page.extract_text()\n",
    "        pagenumber_text.append({\"pagenumber\": i+1, \"text\": text})\n",
    "\n",
    "\n",
    "output_path = Path(\"../data/processed/td_2024_pages.json\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(pagenumber_text , f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print('Saved to:', output_path)\n",
    "\n",
    "len (pagenumber_text)\n",
    "# for page in pagenumber_text[:2]:\n",
    "#     print(f\"\\n---page{page['pagenumber']}---\\n\")\n",
    "#     print(page['text'][:1000])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959414b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text (text:str) -> str:\n",
    "\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "\n",
    "    text = re.sub(r\"[ ]{2,}\", \" \", text)\n",
    "\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "for page in pagenumber_text:\n",
    "\n",
    "    page[\"text_clean\"] = clean_text(page[\"text\"])\n",
    "\n",
    "\n",
    "print (\"raw:\\n\" + pagenumber_text[17][\"text\"][:500])\n",
    "print (\"\\nclean:\\n\" + pagenumber_text[17][\"text_clean\"][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce0f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def normalize_header_footer(line: str) -> str:\n",
    "    line = line.strip()\n",
    "\n",
    "    # remove leading page number like \"10 TD BANK ...\"\n",
    "    line = re.sub(r\"^\\d+\\s+\", \"\", line)\n",
    "\n",
    "    # remove trailing page number like \"... OUR STRATEGY 10\"\n",
    "    line = re.sub(r\"\\s+\\d+\\s*$\", \"\", line)\n",
    "\n",
    "    # collapse multiple spaces\n",
    "    line = re.sub(r\"\\s{2,}\", \" \", line)\n",
    "\n",
    "    return line\n",
    "\n",
    "def collect_candidates(pagenumber_text, n=3):\n",
    "    header_lines = []\n",
    "    footer_lines = []\n",
    "\n",
    "    for page in pagenumber_text:\n",
    "        text = page.get(\"text_clean\") or page.get(\"text\") or \"\"\n",
    "        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "        if not lines:\n",
    "            continue\n",
    "\n",
    "        header_lines.extend([normalize_header_footer(ln) for ln in lines[:n]])\n",
    "        footer_lines.extend([normalize_header_footer(ln) for ln in lines[-n:]])\n",
    "\n",
    "    return Counter(header_lines), Counter(footer_lines)\n",
    "\n",
    "header_counts, footer_counts = collect_candidates(pagenumber_text, n=3)\n",
    "\n",
    "print(\"Top header candidates:\")\n",
    "for line, count in header_counts.most_common(10):\n",
    "    print(f\"{count:3d} | {line}\")\n",
    "\n",
    "print(\"\\nTop footer candidates:\")\n",
    "for line, count in footer_counts.most_common(10):\n",
    "    print(f\"{count:3d} | {line}\")\n",
    "\n",
    "HEADER_MIN_COUNT = 10\n",
    "FOOTER_MIN_COUNT = 10\n",
    "\n",
    "header_lines_to_remove = {line for line, cnt in header_counts.items() if cnt >= HEADER_MIN_COUNT}\n",
    "footer_lines_to_remove = {line for line, cnt in footer_counts.items() if cnt >= FOOTER_MIN_COUNT}\n",
    "\n",
    "lines_to_remove = header_lines_to_remove | footer_lines_to_remove\n",
    "print(\"\\n# lines_to_remove =\", len(lines_to_remove))\n",
    "\n",
    "def remove_repeated_headers_footers(text: str, remove_set: set[str]) -> str:\n",
    "    kept = []\n",
    "    for ln in text.splitlines():\n",
    "        ln_norm = normalize_header_footer(ln)\n",
    "        if ln_norm in remove_set:\n",
    "            continue\n",
    "        kept.append(ln)\n",
    "    return \"\\n\".join(kept)\n",
    "\n",
    "for page in pagenumber_text:\n",
    "    base = page.get(\"text_clean\") or page.get(\"text\") or \"\"\n",
    "    page[\"text_nostruct\"] = remove_repeated_headers_footers(base, lines_to_remove)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bc306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check\n",
    "i = 11\n",
    "print(\"BEFORE:\\n\", pagenumber_text[i][\"text_clean\"][:250], \"\\n...\\n\", pagenumber_text[i][\"text_clean\"][-250:])\n",
    "print(\"\\nAFTER:\\n\", pagenumber_text[i][\"text_nostruct\"][:250], \"\\n...\\n\", pagenumber_text[i][\"text_nostruct\"][-250:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d6bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for page in pagenumber_text:\n",
    "    page[\"text_final\"] = page.get(\"text_nostruct\") or page.get(\"text_clean\") or page.get(\"text\") or \"\"\n",
    "\n",
    "\n",
    "pages_final = []\n",
    "for page in pagenumber_text:\n",
    "    pages_final.append({\n",
    "        \"pagenumber\": page[\"pagenumber\"],\n",
    "        \"text_final\": page[\"text_final\"].strip()\n",
    "    })\n",
    "\n",
    "\n",
    "pages_final = [p for p in pages_final if p[\"text_final\"]]\n",
    "\n",
    "print(\"Final pages:\", len(pages_final))\n",
    "\n",
    "\n",
    "out_path = Path(\"../data/processed/td_2024_pages_final.json\")\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(pages_final, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58914ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_spaces(s: str) -> str:\n",
    "    s = s.replace(\"\\x00\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def chunk_text(text: str, chunk_size=1200, overlap=200):\n",
    "    \"\"\"\n",
    "    Simple character-based chunking with overlap.\n",
    "    Works well as a first version.\n",
    "    \"\"\"\n",
    "    text = normalize_spaces(text)\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "\n",
    "    while start < n:\n",
    "        end = min(start + chunk_size, n)\n",
    "        chunks.append(text[start:end])\n",
    "        if end == n:\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "all_chunks = []\n",
    "for page in pagenumber_text:\n",
    "    final_text = page.get(\"text_final\") or \"\"\n",
    "    chunks = chunk_text(final_text, chunk_size=1200, overlap=200)\n",
    "\n",
    "    page[\"chunks\"] = chunks  \n",
    "\n",
    "    \n",
    "    for j, ch in enumerate(chunks):\n",
    "        all_chunks.append({\n",
    "            \"pdf_page\": page[\"pagenumber\"],\n",
    "            \"chunk_id\": j,\n",
    "            \"text\": ch\n",
    "        })\n",
    "\n",
    "len(all_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3607057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks[11][\"pdf_page\"], len(all_chunks[11][\"text\"]), all_chunks[11][\"text\"][:1200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93fa035",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "out_chunks = Path(\"../data/processed/td_2024_chunks.json\")\n",
    "\n",
    "out_chunks.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(out_chunks, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Saved:\", out_chunks)\n",
    "print(\"Total chunks:\", len(all_chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e45ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in all_chunks[:3]:\n",
    "    print(r[\"pdf_page\"], r[\"chunk_id\"], \"=>\", r[\"text\"][:120], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45582043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "chunks_path = Path(\"../data/processed/td_2024_chunks.json\")\n",
    "all_chunks = json.loads(chunks_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "len(all_chunks), all_chunks[0].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a0efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI Embeddings\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "# assert os.getenv(\"OPENAI_API_KEY\"), \"Missing OPENAI_API_KEY in .env\"\n",
    "print(\"Key loaded:\", os.getenv(\"OPENAI_API_KEY\") is not None)\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "persist_dir = Path(\"../data/processed/chroma_td2024\")\n",
    "persist_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\"\n",
    "                       \n",
    "                       )\n",
    "\n",
    "\n",
    "texts = [c[\"text\"] for c in all_chunks]\n",
    "metadatas = [{\"bank\": \"TD\", \"year\": 2024, \"pdf_page\": c[\"pdf_page\"], \"chunk_id\": c[\"chunk_id\"]} for c in all_chunks]\n",
    "\n",
    "lengths = [len(t) for t in texts]\n",
    "print(\"max len:\", max(lengths), \"avg:\", sum(lengths)/len(lengths))\n",
    "print(\"top 5:\", sorted(lengths, reverse=True)[:5])\n",
    "\n",
    "vectordb = Chroma.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=emb,\n",
    "    metadatas=metadatas,\n",
    "    persist_directory=str(persist_dir),\n",
    "    collection_name=\"bank_reports\",\n",
    ")\n",
    "\n",
    "vectordb.persist()\n",
    "print(\"Saved ChromaDB to:\", persist_dir)\n",
    "print(\"Docs stored:\", len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878cc36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VertexAI Embeddings\n",
    "\n",
    "from pathlib import Path\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vec_path = Path(\"../data/processed/chroma_td2024_vertex\")\n",
    "vec_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "emb_v = VertexAIEmbeddings(model_name=\"text-embedding-001\",\n",
    "                           project=\"bank-report-rag\",\n",
    "                           location=\"us-central1\")\n",
    "\n",
    "\n",
    "texts = [c[\"text\"] for c in all_chunks]\n",
    "metadatas = [{\"bank\": \"TD\", \"year\": 2024, \"pdf_page\": c[\"pdf_page\"], \"chunk_id\": c[\"chunk_id\"]} for c in all_chunks]\n",
    "\n",
    "\n",
    "vectordb_v = Chroma(\n",
    "    persist_directory=str(vec_path),\n",
    "    collection_name=\"bank_reports\",\n",
    "    embedding_function=emb_v,\n",
    ")\n",
    "\n",
    "BATCH = 50\n",
    "for i in range(0, len(texts), BATCH):\n",
    "    batch_texts = texts[i:i+BATCH]\n",
    "    batch_metas = metadatas[i:i+BATCH]\n",
    "    try:\n",
    "        vectordb_v.add_texts(texts=batch_texts, metadatas=batch_metas)\n",
    "        print(f\"Added {min(i+BATCH, len(texts))}/{len(texts)}\")\n",
    "    except Exception as e:\n",
    "        lens = [len(t) for t in batch_texts]\n",
    "        j = max(range(len(lens)), key=lambda k: lens[k])\n",
    "        print(\"FAILED BATCH starting at\", i)\n",
    "        print(\"Longest text len:\", lens[j])\n",
    "        print(\"Metadata:\", batch_metas[j])\n",
    "        raise\n",
    "\n",
    "\n",
    "vectordb_v.persist()\n",
    "print(\"Saved ChromaDB to:\", vec_path)\n",
    "print(\"Docs stored:\", len(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2f6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectordb.similarity_search(\n",
    "    \"risk factors\",\n",
    "    k=5,\n",
    "    filter={\n",
    "        \"$and\": [\n",
    "            {\"bank\": \"TD\"},\n",
    "            {\"year\": 2024}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d521cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "                       \n",
    "vec_path = Path(\"../data/processed/chroma_td2024\")\n",
    "vectordb = Chroma(\n",
    "    persist_directory=str(vec_path),\n",
    "     collection_name=\"bank_reports\",  \n",
    "    embedding_function=emb\n",
    ")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "\n",
    "def answer_question(question, bank=\"TD\", year=2024, k=5, show_snippets=True):\n",
    "    retriever = vectordb.as_retriever(\n",
    "        search_kwargs={\"k\":k, \"filter\":{\"$and\": [\n",
    "        {\"bank\": \"TD\"},\n",
    "        {\"year\": 2024}\n",
    "        ]\n",
    "        }}\n",
    "    )\n",
    "\n",
    "    docs = retriever.invoke(question)   \n",
    "\n",
    "  \n",
    "    seen = set()\n",
    "    unique_docs = []\n",
    "    for d in docs:\n",
    "        md = d.metadata or {}\n",
    "        key = (md.get(\"pdf_page\"), md.get(\"chunk_id\"))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        unique_docs.append(d)\n",
    "\n",
    "    context_blocks = []\n",
    "    sources = []\n",
    "\n",
    "    for d in unique_docs:\n",
    "        md = d.metadata or {}\n",
    "        pg = md.get(\"pdf_page\")\n",
    "        cid = md.get(\"chunk_id\")\n",
    "        text = d.page_content\n",
    "\n",
    "        context_blocks.append(f\"[page {pg} | chunk {cid}]\\n{text}\")\n",
    "\n",
    "        src = {\"pdf_page\": pg, \"chunk_id\": cid}\n",
    "        if show_snippets:\n",
    "            src[\"snippet\"] = text[:220].replace(\"\\n\", \" \") + \"...\"\n",
    "        sources.append(src)\n",
    "\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_blocks)\n",
    "\n",
    "    prompt = f\"\"\"You are a helpful analyst reading a bank annual report.\n",
    "Use ONLY the provided context.\n",
    "If the answer is not in the context, say: \"I do not know based on the provided text.\"\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "Return:\n",
    "1) Answer (short, clear)\n",
    "2) Bullet list of key evidence (1-4 bullets)\n",
    "\"\"\"\n",
    "\n",
    "    resp = llm.invoke(prompt)\n",
    "    return {\"answer\": resp.content, \"sources\": sources}\n",
    "\n",
    "\n",
    "result = answer_question(\n",
    "    \"What are the main risk factors mentioned for 2024?\",\n",
    "    bank=\"TD\",\n",
    "    year=2024,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(result[\"answer\"])\n",
    "print(\"\\nSOURCES:\", result[\"sources\"])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
