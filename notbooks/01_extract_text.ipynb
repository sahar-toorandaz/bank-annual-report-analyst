{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9b8d4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key loaded: True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(\"Key loaded:\", OPENAI_API_KEY is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d4364aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: ..\\data\\processed\\td_2024_pages.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "pdf_path = Path(\"../data/raw/TD-2024-Annual-Report.pdf\")\n",
    "\n",
    "pagenumber_text = []\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for i , page in enumerate (pdf.pages):\n",
    "        text = page.extract_text()\n",
    "        pagenumber_text.append({\"pagenumber\": i+1, \"text\": text})\n",
    "\n",
    "\n",
    "output_path = Path(\"../data/processed/td_2024_pages.json\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(pagenumber_text , f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print('Saved to:', output_path)\n",
    "\n",
    "len (pagenumber_text)\n",
    "# for page in pagenumber_text[:2]:\n",
    "#     print(f\"\\n---page{page['pagenumber']}---\\n\")\n",
    "#     print(page['text'][:1000])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "959414b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw:\n",
      "Board Committees\n",
      "COMMITTEE MEMBERS1 KEY RESPONSIBILITIES2\n",
      "Corporate Alan N. MacGibbon Responsibility for corporate governance of the Bank:\n",
      "Governance (Chair) • Identify individuals qualified to become Board members, recommend to the Board the director\n",
      "Committee Amy W. Brinkley nominees for the next annual meeting of shareholders and recommend candidates to fill vacancies\n",
      "Claude Mongeau on the Board that occur between meetings of the shareholders.\n",
      "Nancy G. Tower • Develop and recommend to the Boa\n",
      "\n",
      "clean:\n",
      "Board Committees\n",
      "COMMITTEE MEMBERS1 KEY RESPONSIBILITIES2\n",
      "Corporate Alan N. MacGibbon Responsibility for corporate governance of the Bank:\n",
      "Governance (Chair) • Identify individuals qualified to become Board members, recommend to the Board the director\n",
      "Committee Amy W. Brinkley nominees for the next annual meeting of shareholders and recommend candidates to fill vacancies\n",
      "Claude Mongeau on the Board that occur between meetings of the shareholders.\n",
      "Nancy G. Tower • Develop and recommend to the Boa\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text (text:str) -> str:\n",
    "\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "\n",
    "    text = re.sub(r\"[ ]{2,}\", \" \", text)\n",
    "\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "for page in pagenumber_text:\n",
    "\n",
    "    page[\"text_clean\"] = clean_text(page[\"text\"])\n",
    "\n",
    "\n",
    "print (\"raw:\\n\" + pagenumber_text[17][\"text\"][:500])\n",
    "print (\"\\nclean:\\n\" + pagenumber_text[17][\"text_clean\"][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce0f019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top header candidates:\n",
      " 28 | |\n",
      " 13 | (millions of Canadian dollars) As at\n",
      "  6 | October 31 October 31 October 31 October\n",
      "  6 | October 31, 2024 October 31,\n",
      "  5 | (millions of Canadian dollars, except as noted) 2024\n",
      "  5 | FINANCIAL RESULTS OVERVIEW\n",
      "  5 | BUSINESS SEGMENT ANALYSIS\n",
      "  5 | BUSINESS STRATEGY BUSINESS HIGHLIGHTS IN\n",
      "  5 | 2023\n",
      "  4 | Order/Agreement Key Requirements\n",
      "\n",
      "Top footer candidates:\n",
      "125 | TD BANK GROUP ANNUAL REPORT 2024 MANAGEMENT’S DISCUSSION AND ANALYSIS\n",
      " 92 | TD BANK GROUP ANNUAL REPORT 2024 FINANCIAL RESULTS\n",
      " 16 | TD BANK GROUP ANNUAL REPORT 2024 OUR STRATEGY\n",
      "  5 | section of this document.\n",
      "  4 | For additional information about the Bank’s use of non-GAAP financial measures, refer to “Non-GAAP and Other Financial Measures” in the “Financial Results Overview”\n",
      "  4 | Includes loans that are measured at FVOCI.\n",
      "  3 | TD BANK GROUP ANNUAL REPORT 2024 GLOSSARY\n",
      "  3 | The accompanying Notes are an integral part of these Consolidated\n",
      "  3 | Financial Statements.\n",
      "  3 | TD BANK GROUP ANNUAL REPORT 2024 TEN-YEAR STATISTICAL REVIEW\n",
      "\n",
      "# lines_to_remove = 5\n",
      "BEFORE:\n",
      " Sustainability\n",
      "As a global financial institution, we know we have\n",
      "an important role to play in supporting our customers,\n",
      "colleagues, and communities in a changing world.\n",
      "Awards and\n",
      "milestones TD’s commitment to sustainability is reflected in\n",
      "our appr \n",
      "...\n",
      "  released an updated financial education initiatives\n",
      "Sustainable Financing Framework. in Canada and the U.S. The\n",
      "progress made on these targets\n",
      "in 2024 will be shared in our 2024\n",
      "Sustainability Report.\n",
      "10 TD BANK GROUP ANNUAL REPORT 2024 OUR STRATEGY\n",
      "\n",
      "AFTER:\n",
      " Sustainability\n",
      "As a global financial institution, we know we have\n",
      "an important role to play in supporting our customers,\n",
      "colleagues, and communities in a changing world.\n",
      "Awards and\n",
      "milestones TD’s commitment to sustainability is reflected in\n",
      "our appr \n",
      "...\n",
      " d this through TD led and supported\n",
      "year, TD also released an updated financial education initiatives\n",
      "Sustainable Financing Framework. in Canada and the U.S. The\n",
      "progress made on these targets\n",
      "in 2024 will be shared in our 2024\n",
      "Sustainability Report.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def normalize_header_footer(line: str) -> str:\n",
    "    line = line.strip()\n",
    "\n",
    "    # remove leading page number like \"10 TD BANK ...\"\n",
    "    line = re.sub(r\"^\\d+\\s+\", \"\", line)\n",
    "\n",
    "    # remove trailing page number like \"... OUR STRATEGY 10\"\n",
    "    line = re.sub(r\"\\s+\\d+\\s*$\", \"\", line)\n",
    "\n",
    "    # collapse multiple spaces\n",
    "    line = re.sub(r\"\\s{2,}\", \" \", line)\n",
    "\n",
    "    return line\n",
    "\n",
    "def collect_candidates(pagenumber_text, n=3):\n",
    "    header_lines = []\n",
    "    footer_lines = []\n",
    "\n",
    "    for page in pagenumber_text:\n",
    "        text = page.get(\"text_clean\") or page.get(\"text\") or \"\"\n",
    "        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "        if not lines:\n",
    "            continue\n",
    "\n",
    "        header_lines.extend([normalize_header_footer(ln) for ln in lines[:n]])\n",
    "        footer_lines.extend([normalize_header_footer(ln) for ln in lines[-n:]])\n",
    "\n",
    "    return Counter(header_lines), Counter(footer_lines)\n",
    "\n",
    "header_counts, footer_counts = collect_candidates(pagenumber_text, n=3)\n",
    "\n",
    "print(\"Top header candidates:\")\n",
    "for line, count in header_counts.most_common(10):\n",
    "    print(f\"{count:3d} | {line}\")\n",
    "\n",
    "print(\"\\nTop footer candidates:\")\n",
    "for line, count in footer_counts.most_common(10):\n",
    "    print(f\"{count:3d} | {line}\")\n",
    "\n",
    "HEADER_MIN_COUNT = 10\n",
    "FOOTER_MIN_COUNT = 10\n",
    "\n",
    "header_lines_to_remove = {line for line, cnt in header_counts.items() if cnt >= HEADER_MIN_COUNT}\n",
    "footer_lines_to_remove = {line for line, cnt in footer_counts.items() if cnt >= FOOTER_MIN_COUNT}\n",
    "\n",
    "lines_to_remove = header_lines_to_remove | footer_lines_to_remove\n",
    "print(\"\\n# lines_to_remove =\", len(lines_to_remove))\n",
    "\n",
    "def remove_repeated_headers_footers(text: str, remove_set: set[str]) -> str:\n",
    "    kept = []\n",
    "    for ln in text.splitlines():\n",
    "        ln_norm = normalize_header_footer(ln)\n",
    "        if ln_norm in remove_set:\n",
    "            continue\n",
    "        kept.append(ln)\n",
    "    return \"\\n\".join(kept)\n",
    "\n",
    "for page in pagenumber_text:\n",
    "    base = page.get(\"text_clean\") or page.get(\"text\") or \"\"\n",
    "    page[\"text_nostruct\"] = remove_repeated_headers_footers(base, lines_to_remove)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91bc306e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:\n",
      " Sustainability\n",
      "As a global financial institution, we know we have\n",
      "an important role to play in supporting our customers,\n",
      "colleagues, and communities in a changing world.\n",
      "Awards and\n",
      "milestones TD’s commitment to sustainability is reflected in\n",
      "our appr \n",
      "...\n",
      "  released an updated financial education initiatives\n",
      "Sustainable Financing Framework. in Canada and the U.S. The\n",
      "progress made on these targets\n",
      "in 2024 will be shared in our 2024\n",
      "Sustainability Report.\n",
      "10 TD BANK GROUP ANNUAL REPORT 2024 OUR STRATEGY\n",
      "\n",
      "AFTER:\n",
      " Sustainability\n",
      "As a global financial institution, we know we have\n",
      "an important role to play in supporting our customers,\n",
      "colleagues, and communities in a changing world.\n",
      "Awards and\n",
      "milestones TD’s commitment to sustainability is reflected in\n",
      "our appr \n",
      "...\n",
      " d this through TD led and supported\n",
      "year, TD also released an updated financial education initiatives\n",
      "Sustainable Financing Framework. in Canada and the U.S. The\n",
      "progress made on these targets\n",
      "in 2024 will be shared in our 2024\n",
      "Sustainability Report.\n"
     ]
    }
   ],
   "source": [
    "# quick check\n",
    "i = 11\n",
    "print(\"BEFORE:\\n\", pagenumber_text[i][\"text_clean\"][:250], \"\\n...\\n\", pagenumber_text[i][\"text_clean\"][-250:])\n",
    "print(\"\\nAFTER:\\n\", pagenumber_text[i][\"text_nostruct\"][:250], \"\\n...\\n\", pagenumber_text[i][\"text_nostruct\"][-250:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8d6bcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final pages: 244\n",
      "Saved: ..\\data\\processed\\td_2024_pages_final.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for page in pagenumber_text:\n",
    "    page[\"text_final\"] = page.get(\"text_nostruct\") or page.get(\"text_clean\") or page.get(\"text\") or \"\"\n",
    "\n",
    "\n",
    "pages_final = []\n",
    "for page in pagenumber_text:\n",
    "    pages_final.append({\n",
    "        \"pagenumber\": page[\"pagenumber\"],\n",
    "        \"text_final\": page[\"text_final\"].strip()\n",
    "    })\n",
    "\n",
    "\n",
    "pages_final = [p for p in pages_final if p[\"text_final\"]]\n",
    "\n",
    "print(\"Final pages:\", len(pages_final))\n",
    "\n",
    "\n",
    "out_path = Path(\"../data/processed/td_2024_pages_final.json\")\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(pages_final, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58914ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def normalize_spaces(s: str) -> str:\n",
    "    s = s.replace(\"\\x00\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def chunk_text(text: str, chunk_size=1200, overlap=200):\n",
    "    \"\"\"\n",
    "    Simple character-based chunking with overlap.\n",
    "    Works well as a first version.\n",
    "    \"\"\"\n",
    "    text = normalize_spaces(text)\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "\n",
    "    while start < n:\n",
    "        end = min(start + chunk_size, n)\n",
    "        chunks.append(text[start:end])\n",
    "        if end == n:\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "all_chunks = []\n",
    "for page in pagenumber_text:\n",
    "    final_text = page.get(\"text_final\") or \"\"\n",
    "    chunks = chunk_text(final_text, chunk_size=1200, overlap=200)\n",
    "\n",
    "    page[\"chunks\"] = chunks  \n",
    "\n",
    "    \n",
    "    for j, ch in enumerate(chunks):\n",
    "        all_chunks.append({\n",
    "            \"pdf_page\": page[\"pagenumber\"],\n",
    "            \"chunk_id\": j,\n",
    "            \"text\": ch\n",
    "        })\n",
    "\n",
    "len(all_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3607057e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " 1200,\n",
       " ' Canada’s top 100 Employers for the 18th consecutive year and America’s Best Employers This was a year with difficult challenges. for Diversity by Forbes for the third year in a row. In addition, The deficiencies of our U.S. AML program were serious. As a TD once again achieved the Great Place to Work certification Global Systemically Important Bank (G-SIB), and an integral in both Canada and the U.S. part of the financial system, we have a responsibility to Our colleagues also advanced programs that build on our protect the system and thwart criminal activity. We did not capabilities to innovate. TD Invent, the Bank’s enterprise deliver, and we apologize to all our stakeholders. approach to innovation, surpassed 10,000 implemented In October, we reached a resolution of these matters with ideas from colleagues across the Bank. And our patent U.S. regulators as well as the Department of Justice. The portfolio reached over 2,500 patents, with more than 800 terms were costly and imposed certain limitations on our U.S. pertaining to artificial intelligence. Driven by colleague retail business, along with significant program remediation ideation, TD is the top patent filer among Canadia')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chunks[11][\"pdf_page\"], len(all_chunks[11][\"text\"]), all_chunks[11][\"text\"][:1200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b93fa035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ..\\data\\processed\\td_2024_chunks.json\n",
      "Total chunks: 1300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "out_chunks = Path(\"../data/processed/td_2024_chunks.json\")\n",
    "\n",
    "out_chunks.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(out_chunks, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Saved:\", out_chunks)\n",
    "print(\"Total chunks:\", len(all_chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e45ee43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 => 2024 Annual Report ...\n",
      "\n",
      "2 0 => Table of Contents OUR STRATEGY 1 Group President and CEO’s Message 2 Chair of the Board’s Message 3 Progress on Our U.S. ...\n",
      "\n",
      "3 0 => Our Strategy Anchored in our proven business Proven Business Model model, we are guided by our Deliver consistent earnin ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in all_chunks[:3]:\n",
    "    print(r[\"pdf_page\"], r[\"chunk_id\"], \"=>\", r[\"text\"][:120], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45582043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1300, dict_keys(['pdf_page', 'chunk_id', 'text']))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "chunks_path = Path(\"../data/processed/td_2024_chunks.json\")\n",
    "all_chunks = json.loads(chunks_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "len(all_chunks), all_chunks[0].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17a0efe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key loaded: True\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Request too large for text-embedding-3-small in organization org-tgYINwnqYzEyzRuEy2DslJoc on tokens per min (TPM): Limit 40000, Requested 83100. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m texts = [c[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m all_chunks]\n\u001b[32m     23\u001b[39m metadatas = [{\u001b[33m\"\u001b[39m\u001b[33mbank\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mTD\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m2024\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpdf_page\u001b[39m\u001b[33m\"\u001b[39m: c[\u001b[33m\"\u001b[39m\u001b[33mpdf_page\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mchunk_id\u001b[39m\u001b[33m\"\u001b[39m: c[\u001b[33m\"\u001b[39m\u001b[33mchunk_id\u001b[39m\u001b[33m\"\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m all_chunks]\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m vectordb = \u001b[43mChroma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpersist_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbank_reports\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m vectordb.persist()\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaved ChromaDB to:\u001b[39m\u001b[33m\"\u001b[39m, persist_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\inst\\Documents\\GitHub\\bank-annual-report-analyst\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:843\u001b[39m, in \u001b[36mChroma.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    835\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbatch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[32m    837\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[32m    838\u001b[39m         api=chroma_collection._client,\n\u001b[32m    839\u001b[39m         ids=ids,\n\u001b[32m    840\u001b[39m         metadatas=metadatas,\n\u001b[32m    841\u001b[39m         documents=texts,\n\u001b[32m    842\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m         \u001b[43mchroma_collection\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    849\u001b[39m     chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\inst\\Documents\\GitHub\\bank-annual-report-analyst\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:277\u001b[39m, in \u001b[36mChroma.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m texts = \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[32m    279\u001b[39m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[32m    280\u001b[39m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[32m    281\u001b[39m     length_diff = \u001b[38;5;28mlen\u001b[39m(texts) - \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\inst\\Documents\\GitHub\\bank-annual-report-analyst\\.venv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:709\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    706\u001b[39m \u001b[38;5;66;03m# Unconditionally call _get_len_safe_embeddings to handle length safety.\u001b[39;00m\n\u001b[32m    707\u001b[39m \u001b[38;5;66;03m# This could be optimized to avoid double work when all texts are short enough.\u001b[39;00m\n\u001b[32m    708\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m709\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\inst\\Documents\\GitHub\\bank-annual-report-analyst\\.venv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:576\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    574\u001b[39m \u001b[38;5;66;03m# Make API call with this batch\u001b[39;00m\n\u001b[32m    575\u001b[39m batch_tokens = tokens[i:batch_end]\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    578\u001b[39m     response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\inst\\Documents\\GitHub\\bank-annual-report-analyst\\.venv\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\inst\\Documents\\GitHub\\bank-annual-report-analyst\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\inst\\Documents\\GitHub\\bank-annual-report-analyst\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'Request too large for text-embedding-3-small in organization org-tgYINwnqYzEyzRuEy2DslJoc on tokens per min (TPM): Limit 40000, Requested 83100. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "# assert os.getenv(\"OPENAI_API_KEY\"), \"Missing OPENAI_API_KEY in .env\"\n",
    "print(\"Key loaded:\", os.getenv(\"OPENAI_API_KEY\") is not None)\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "persist_dir = Path(\"../data/processed/chroma_td2024\")\n",
    "persist_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\",\n",
    "                       openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "                       )\n",
    "\n",
    "\n",
    "texts = [c[\"text\"] for c in all_chunks]\n",
    "metadatas = [{\"bank\": \"TD\", \"year\": 2024, \"pdf_page\": c[\"pdf_page\"], \"chunk_id\": c[\"chunk_id\"]} for c in all_chunks]\n",
    "\n",
    "vectordb = Chroma.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=emb,\n",
    "    metadatas=metadatas,\n",
    "    persist_directory=str(persist_dir),\n",
    "    collection_name=\"bank_reports\",\n",
    ")\n",
    "\n",
    "vectordb.persist()\n",
    "print(\"Saved ChromaDB to:\", persist_dir)\n",
    "print(\"Docs stored:\", len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fafb92b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
